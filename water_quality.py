# -*- coding: utf-8 -*-
"""Water Quality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nvlQDWVlwXv2rQXoyGxx0Ta2OOxttoyd
"""

import pandas as pd

data = pd.read_csv("water_potability.csv")

data

data.describe()

data.info()

data.head()

data.tail()

data.isnull().sum()

data.shape

data.columns

data.nunique()

data.dtypes

data.corr()["Potability"].abs().sort_values(ascending=False)*100

import seaborn as sns

ax=sns.countplot(data['Potability'])
plt.xticks(ticks=[0,1],labels=['Non portable','portable'])
plt.show()

sns.pairplot(data,hue='Potability')

data.hist(column='ph',by='Potability')

data.hist(column='Hardness',by='Potability')

import plotly.express as px

fig=px.pie(data,names='Potability',hole=0.4,template='plotly_dark')
fig.show()

data.isna().sum()

data.isnull().mean().plot.bar(figsize=[10,6])

data['ph']=data['ph'].fillna(data['ph']).mean()

data['Sulfate']=data['Sulfate'].fillna(data['Sulfate']).mean()
data['Trihalomethanes']=data['Trihalomethanes'].fillna(data['Trihalomethanes']).mean()

data.isnull().sum()

data.isna().mean().plot.bar(figsize=[10,6])

data.head()

data.describe()

X=data.drop('Potability',axis=1)
y=data['Potability']

X.shape,y.shape

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()

X=scaler.fit_transform(X)

X

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=30,random_state=0)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

model_kn = KNeighborsClassifier()

model_kn.fit(X_train,y_train)

pred_kn = model_kn.predict(X_test)

kn_accuracy = accuracy_score(y_test,pred_kn)

kn_accuracy

print(classification_report(y_test,pred_kn))

import numpy as np

cm=confusion_matrix(y_test,pred_kn)
sns.heatmap(cm/np.sum(cm),annot=True,fmt='0.2%' , cmap='Reds')

from sklearn.svm import SVC,LinearSVC

model_svm = SVC()

model_svm.fit(X_train,y_train)

pred_svm = model_svm.predict(X_test)

sv_accuracy = accuracy_score(y_test,pred_svm)
sv_accuracy

print(classification_report(y_test,pred_svm))

svm_cm = confusion_matrix(y_test,pred_svm)
sns.heatmap(svm_cm/np.sum(svm_cm), annot=True, fmt='0.2%' , cmap='Reds')

!pip install lightgbm

from lightgbm import LGBMClassifier

# Create the LightGBM classifier
lgbm = LGBMClassifier()

# Train the classifier
lgbm.fit(X_train, y_train)

# Make predictions on the test set
y_pred = lgbm.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print('Accuracy:', accuracy)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# Create the classifiers
rf = RandomForestClassifier()
gbc = GradientBoostingClassifier()
ada = DecisionTreeClassifier()
svm = SVC()

# Train the classifiers
rf.fit(X_train, y_train)
gbc.fit(X_train, y_train)
ada.fit(X_train, y_train)

# Make predictions on the test set
rf_pred = rf.predict(X_test)
gbc_pred = gbc.predict(X_test)
ada_pred = ada.predict(X_test)
#svm_pred = svm.predict(X_test)

# Calculate the accuracy of each classifier
rf_accuracy = accuracy_score(y_test, rf_pred)
gbc_accuracy = accuracy_score(y_test, gbc_pred)
ada_accuracy = accuracy_score(y_test, ada_pred)
#svm_accuracy = accuracy_score(y_test, svm_pred)

# Print the accuracy of each classifier
print('Random Forest accuracy:', rf_accuracy)
print('Gradient Boosting Classifier accuracy:', gbc_accuracy)
print('AdaBoost Classifier accuracy:', ada_accuracy)
#print('Support Vector Machine accuracy:', svm_accuracy)

#!pip install xgboost

#!pip install catboost

from sklearn.ensemble import ExtraTreesClassifier
import xgboost as xgb
import catboost as catboost
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from xgboost import XGBClassifier

# Create the classifiers
et = ExtraTreesClassifier()
xgb = xgb.XGBClassifier()
cat = catboost.CatBoostClassifier()

# Train the classifiers
et.fit(X_train, y_train)
xgb.fit(X_train, y_train)
cat.fit(X_train, y_train)

# Create a deep neural network
dnn = Sequential()
dnn.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))
dnn.add(Dense(64, activation='relu'))
dnn.add(Dense(1, activation='sigmoid'))
dnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the deep neural network
dnn.fit(X_train, y_train, epochs=100)

# Make predictions on the test set
et_pred = et.predict(X_test)
xgb_pred = xgb.predict(X_test)
cat_pred = cat.predict(X_test)
dnn_pred = dnn.predict(X_test)



# Calculate the accuracy of each classifier
et_accuracy = accuracy_score(y_test, et_pred)

xgb_accuracy = accuracy_score(y_test, xgb_pred)

cat_accuracy = accuracy_score(y_test, cat_pred)

# Print the accuracy of each classifier
print('ExtraTreesClassifier accuracy:', et_accuracy)
print('XGBoost accuracy:', xgb_accuracy)
print('CatBoost accuracy:', cat_accuracy)

dnn_accuracy = accuracy_score(y_test, dnn_pred)

dnn_accuracy

models =  pd.DataFrame({'Model':['KNeighborsClassifier','SVM','LGBM','RandomForest','Gradient Boosting Classifier','AdaBoost Classifier accuracy:','ExtraTreesClassifier accuracy:','XGBoost accuracy:','CatBoost accuracy:'],"Accuracy_Score":[kn_accuracy,sv_accuracy,accuracy,rf_accuracy,gbc_accuracy,ada_accuracy,et_accuracy,xgb_accuracy,cat_accuracy]})
models

sns.barplot(x='Accuracy_Score',y='Model',data=models)

'Thank you'